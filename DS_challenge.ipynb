{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e66e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6493e2",
   "metadata": {},
   "source": [
    "## Ejercicio contar lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04f3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b39bd67",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module bz2:\n",
      "\n",
      "NAME\n",
      "    bz2 - Interface to the libbzip2 compression library.\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.8/library/bz2\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides a file interface, classes for incremental\n",
      "    (de)compression, and functions for one-shot (de)compression.\n",
      "\n",
      "CLASSES\n",
      "    _compression.BaseStream(io.BufferedIOBase)\n",
      "        BZ2File\n",
      "    builtins.object\n",
      "        _bz2.BZ2Compressor\n",
      "        _bz2.BZ2Decompressor\n",
      "    \n",
      "    class BZ2Compressor(builtins.object)\n",
      "     |  BZ2Compressor(compresslevel=9, /)\n",
      "     |  \n",
      "     |  Create a compressor object for compressing data incrementally.\n",
      "     |  \n",
      "     |    compresslevel\n",
      "     |      Compression level, as a number between 1 and 9.\n",
      "     |  \n",
      "     |  For one-shot compression, use the compress() function instead.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  compress(self, data, /)\n",
      "     |      Provide data to the compressor object.\n",
      "     |      \n",
      "     |      Returns a chunk of compressed data if possible, or b'' otherwise.\n",
      "     |      \n",
      "     |      When you have finished providing data to the compressor, call the\n",
      "     |      flush() method to finish the compression process.\n",
      "     |  \n",
      "     |  flush(self, /)\n",
      "     |      Finish the compression process.\n",
      "     |      \n",
      "     |      Returns the compressed data left in internal buffers.\n",
      "     |      \n",
      "     |      The compressor object may not be used after this method is called.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class BZ2Decompressor(builtins.object)\n",
      "     |  Create a decompressor object for decompressing data incrementally.\n",
      "     |  \n",
      "     |  For one-shot decompression, use the decompress() function instead.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decompress(self, /, data, max_length=-1)\n",
      "     |      Decompress *data*, returning uncompressed data as bytes.\n",
      "     |      \n",
      "     |      If *max_length* is nonnegative, returns at most *max_length* bytes of\n",
      "     |      decompressed data. If this limit is reached and further output can be\n",
      "     |      produced, *self.needs_input* will be set to ``False``. In this case, the next\n",
      "     |      call to *decompress()* may provide *data* as b'' to obtain more of the output.\n",
      "     |      \n",
      "     |      If all of the input data was decompressed and returned (either because this\n",
      "     |      was less than *max_length* bytes, or because *max_length* was negative),\n",
      "     |      *self.needs_input* will be set to True.\n",
      "     |      \n",
      "     |      Attempting to decompress data after the end of stream is reached raises an\n",
      "     |      EOFError.  Any data found after the end of the stream is ignored and saved in\n",
      "     |      the unused_data attribute.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  eof\n",
      "     |      True if the end-of-stream marker has been reached.\n",
      "     |  \n",
      "     |  needs_input\n",
      "     |      True if more input is needed before more decompressed data can be produced.\n",
      "     |  \n",
      "     |  unused_data\n",
      "     |      Data found after the end of the compressed stream.\n",
      "    \n",
      "    class BZ2File(_compression.BaseStream)\n",
      "     |  BZ2File(filename, mode='r', buffering=<object object at 0x7f296047e8b0>, compresslevel=9)\n",
      "     |  \n",
      "     |  A file object providing transparent bzip2 (de)compression.\n",
      "     |  \n",
      "     |  A BZ2File can act as a wrapper for an existing file object, or refer\n",
      "     |  directly to a named file on disk.\n",
      "     |  \n",
      "     |  Note that BZ2File provides a *binary* file interface - data read is\n",
      "     |  returned as bytes, and data to be written should be given as bytes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BZ2File\n",
      "     |      _compression.BaseStream\n",
      "     |      io.BufferedIOBase\n",
      "     |      _io._BufferedIOBase\n",
      "     |      io.IOBase\n",
      "     |      _io._IOBase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, filename, mode='r', buffering=<object object at 0x7f296047e8b0>, compresslevel=9)\n",
      "     |      Open a bzip2-compressed file.\n",
      "     |      \n",
      "     |      If filename is a str, bytes, or PathLike object, it gives the\n",
      "     |      name of the file to be opened. Otherwise, it should be a file\n",
      "     |      object, which will be used to read or write the compressed data.\n",
      "     |      \n",
      "     |      mode can be 'r' for reading (default), 'w' for (over)writing,\n",
      "     |      'x' for creating exclusively, or 'a' for appending. These can\n",
      "     |      equivalently be given as 'rb', 'wb', 'xb', and 'ab'.\n",
      "     |      \n",
      "     |      buffering is ignored since Python 3.0. Its use is deprecated.\n",
      "     |      \n",
      "     |      If mode is 'w', 'x' or 'a', compresslevel can be a number between 1\n",
      "     |      and 9 specifying the level of compression: 1 produces the least\n",
      "     |      compression, and 9 (default) produces the most compression.\n",
      "     |      \n",
      "     |      If mode is 'r', the input file may be the concatenation of\n",
      "     |      multiple compressed streams.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Flush and close the file.\n",
      "     |      \n",
      "     |      May be called more than once without error. Once the file is\n",
      "     |      closed, any other operation on it will raise a ValueError.\n",
      "     |  \n",
      "     |  fileno(self)\n",
      "     |      Return the file descriptor for the underlying file.\n",
      "     |  \n",
      "     |  peek(self, n=0)\n",
      "     |      Return buffered data without advancing the file position.\n",
      "     |      \n",
      "     |      Always returns at least one byte of data, unless at EOF.\n",
      "     |      The exact number of bytes returned is unspecified.\n",
      "     |  \n",
      "     |  read(self, size=-1)\n",
      "     |      Read up to size uncompressed bytes from the file.\n",
      "     |      \n",
      "     |      If size is negative or omitted, read until EOF is reached.\n",
      "     |      Returns b'' if the file is already at EOF.\n",
      "     |  \n",
      "     |  read1(self, size=-1)\n",
      "     |      Read up to size uncompressed bytes, while trying to avoid\n",
      "     |      making multiple reads from the underlying stream. Reads up to a\n",
      "     |      buffer's worth of data if size is negative.\n",
      "     |      \n",
      "     |      Returns b'' if the file is at EOF.\n",
      "     |  \n",
      "     |  readable(self)\n",
      "     |      Return whether the file was opened for reading.\n",
      "     |  \n",
      "     |  readinto(self, b)\n",
      "     |      Read bytes into b.\n",
      "     |      \n",
      "     |      Returns the number of bytes read (0 for EOF).\n",
      "     |  \n",
      "     |  readline(self, size=-1)\n",
      "     |      Read a line of uncompressed bytes from the file.\n",
      "     |      \n",
      "     |      The terminating newline (if present) is retained. If size is\n",
      "     |      non-negative, no more than size bytes will be read (in which\n",
      "     |      case the line may be incomplete). Returns b'' if already at EOF.\n",
      "     |  \n",
      "     |  readlines(self, size=-1)\n",
      "     |      Read a list of lines of uncompressed bytes from the file.\n",
      "     |      \n",
      "     |      size can be specified to control the number of lines read: no\n",
      "     |      further lines will be read once the total size of the lines read\n",
      "     |      so far equals or exceeds size.\n",
      "     |  \n",
      "     |  seek(self, offset, whence=0)\n",
      "     |      Change the file position.\n",
      "     |      \n",
      "     |      The new position is specified by offset, relative to the\n",
      "     |      position indicated by whence. Values for whence are:\n",
      "     |      \n",
      "     |          0: start of stream (default); offset must not be negative\n",
      "     |          1: current stream position\n",
      "     |          2: end of stream; offset must not be positive\n",
      "     |      \n",
      "     |      Returns the new file position.\n",
      "     |      \n",
      "     |      Note that seeking is emulated, so depending on the parameters,\n",
      "     |      this operation may be extremely slow.\n",
      "     |  \n",
      "     |  seekable(self)\n",
      "     |      Return whether the file supports seeking.\n",
      "     |  \n",
      "     |  tell(self)\n",
      "     |      Return the current file position.\n",
      "     |  \n",
      "     |  writable(self)\n",
      "     |      Return whether the file was opened for writing.\n",
      "     |  \n",
      "     |  write(self, data)\n",
      "     |      Write a byte string to the file.\n",
      "     |      \n",
      "     |      Returns the number of uncompressed bytes written, which is\n",
      "     |      always len(data). Note that due to buffering, the file on disk\n",
      "     |      may not reflect the data written until close() is called.\n",
      "     |  \n",
      "     |  writelines(self, seq)\n",
      "     |      Write a sequence of byte strings to the file.\n",
      "     |      \n",
      "     |      Returns the number of uncompressed bytes written.\n",
      "     |      seq can be any iterable yielding byte strings.\n",
      "     |      \n",
      "     |      Line separators are not added between the written byte strings.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  closed\n",
      "     |      True if this file is closed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _io._BufferedIOBase:\n",
      "     |  \n",
      "     |  detach(self, /)\n",
      "     |      Disconnect this buffer from its underlying raw stream and return it.\n",
      "     |      \n",
      "     |      After the raw stream has been detached, the buffer is in an unusable\n",
      "     |      state.\n",
      "     |  \n",
      "     |  readinto1(self, buffer, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _io._IOBase:\n",
      "     |  \n",
      "     |  __del__(...)\n",
      "     |  \n",
      "     |  __enter__(...)\n",
      "     |  \n",
      "     |  __exit__(...)\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __next__(self, /)\n",
      "     |      Implement next(self).\n",
      "     |  \n",
      "     |  flush(self, /)\n",
      "     |      Flush write buffers, if applicable.\n",
      "     |      \n",
      "     |      This is not implemented for read-only and non-blocking streams.\n",
      "     |  \n",
      "     |  isatty(self, /)\n",
      "     |      Return whether this is an 'interactive' stream.\n",
      "     |      \n",
      "     |      Return False if it can't be determined.\n",
      "     |  \n",
      "     |  truncate(...)\n",
      "     |      Truncate file to size bytes.\n",
      "     |      \n",
      "     |      File pointer is left unchanged.  Size defaults to the current IO\n",
      "     |      position as reported by tell().  Returns the new size.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from _io._IOBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _io._IOBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "\n",
      "FUNCTIONS\n",
      "    compress(data, compresslevel=9)\n",
      "        Compress a block of data.\n",
      "        \n",
      "        compresslevel, if given, must be a number between 1 and 9.\n",
      "        \n",
      "        For incremental compression, use a BZ2Compressor object instead.\n",
      "    \n",
      "    decompress(data)\n",
      "        Decompress a block of data.\n",
      "        \n",
      "        For incremental decompression, use a BZ2Decompressor object instead.\n",
      "    \n",
      "    open(filename, mode='rb', compresslevel=9, encoding=None, errors=None, newline=None)\n",
      "        Open a bzip2-compressed file in binary or text mode.\n",
      "        \n",
      "        The filename argument can be an actual filename (a str, bytes, or\n",
      "        PathLike object), or an existing file object to read from or write\n",
      "        to.\n",
      "        \n",
      "        The mode argument can be \"r\", \"rb\", \"w\", \"wb\", \"x\", \"xb\", \"a\" or\n",
      "        \"ab\" for binary mode, or \"rt\", \"wt\", \"xt\" or \"at\" for text mode.\n",
      "        The default mode is \"rb\", and the default compresslevel is 9.\n",
      "        \n",
      "        For binary mode, this function is equivalent to the BZ2File\n",
      "        constructor: BZ2File(filename, mode, compresslevel). In this case,\n",
      "        the encoding, errors and newline arguments must not be provided.\n",
      "        \n",
      "        For text mode, a BZ2File object is created, and wrapped in an\n",
      "        io.TextIOWrapper instance with the specified encoding, error\n",
      "        handling behavior, and line ending(s).\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BZ2File', 'BZ2Compressor', 'BZ2Decompressor', 'open', 'com...\n",
      "\n",
      "AUTHOR\n",
      "    Nadeem Vawda <nadeem.vawda@gmail.com>\n",
      "\n",
      "FILE\n",
      "    /home/dsc/anaconda3/lib/python3.8/bz2.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bz2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3290ba",
   "metadata": {},
   "source": [
    "10000011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a98bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dsc/DS_challenge\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c19a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "bzcat: I/O or other error, bailing out.  Possible reason follows.\r\n",
      "bzcat: Broken pipe\r\n",
      "\tInput file = ../data/challenge/bookings.csv.bz2, output file = (stdout)\r\n"
     ]
    }
   ],
   "source": [
    "!bzcat ../data/challenge/bookings.csv.bz2 | head -10000 > bookings_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bbe19e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "bzcat: I/O or other error, bailing out.  Possible reason follows.\r\n",
      "bzcat: Broken pipe\r\n",
      "\tInput file = ../data/challenge/searches.csv.bz2, output file = (stdout)\r\n"
     ]
    }
   ],
   "source": [
    "!bzcat ../data/challenge/searches.csv.bz2 | head -10000 > searches_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d386dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      \r\n",
      "2013-03-05 00:00:00^1A    ^DE      ^a68dd7ae953c8acfb187a1af2dcbe123^1a11ae49fcbf545fd2afc1a24d88d2b7^ea65900e72d71f4626378e2ebd298267^2013-02-22 00:00:00^1708^0^ZRH     ^ZRH     ^CH      ^LHR     ^LON     ^GB      ^ZRH     ^ZRH     ^CH      ^LHR     ^LON     ^GB      ^ZRH     ^ZRH     ^CH      ^LHRZRH  ^LONZRH  ^CHGB    ^1^LHRZRH         ^VI^T        ^Y        ^2013-03-07 08:50:00^2013-03-07 11:33:37^-1^2013^3^NULL     \r\n",
      "2013-03-26 00:00:00^1A    ^US      ^e612b9eeeee6f17f42d9b0d3b79e75ca^7437560d8f276d6d05eeb806d9e7edee^737295a86982c941f1c2da9a46a14043^2013-03-26 00:00:00^135270^0^SAL     ^SAL     ^SV      ^CLT     ^CLT     ^US      ^SAL     ^SAL     ^SV      ^SAL     ^SAL     ^SV      ^CLT     ^CLT     ^US      ^CLTSAL  ^CLTSAL  ^SVUS    ^1^SALATLCLT      ^NV^L        ^Y        ^2013-04-12 13:04:00^2013-04-12 22:05:40^1^2013^3^NULL     \r\n",
      "2013-03-26 00:00:00^1A    ^US      ^e612b9eeeee6f17f42d9b0d3b79e75ca^7437560d8f276d6d05eeb806d9e7edee^737295a86982c941f1c2da9a46a14043^2013-03-26 00:00:00^135270^0^SAL     ^SAL     ^SV      ^CLT     ^CLT     ^US      ^SAL     ^SAL     ^SV      ^CLT     ^CLT     ^US      ^SAL     ^SAL     ^SV      ^CLTSAL  ^CLTSAL  ^SVUS    ^1^CLTATLSAL      ^NV^U        ^Y        ^2013-07-15 07:00:00^2013-07-15 11:34:51^1^2013^3^NULL     \r\n",
      "2013-03-26 00:00:00^1A    ^AU      ^0f984b3bb6bd06661c95529bbd6193bc^36472c6dbaf7afec9136ac40364e2794^5ecf00fdcbcec761c43dc7285253d0c1^2013-03-26 00:00:00^30885^0^AKL     ^AKL     ^NZ      ^SVO     ^MOW     ^RU      ^AKL     ^AKL     ^NZ      ^AKL     ^AKL     ^NZ      ^SVO     ^MOW     ^RU      ^AKLSVO  ^AKLMOW  ^NZRU    ^1^AKLHKGSVO      ^XK^G        ^Y        ^2013-04-24 23:59:00^2013-04-25 16:06:31^1^2013^3^SYDA82546\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 bookings_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009e2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bzip2 -f bookings_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630021d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dsc/DS_challenge\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52cf5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bk = bz2.BZ2File('bookings_sample.csv.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8edd1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with bz2.BZ2File('bookings_sample.csv.bz2') as bk:\n",
    "    for i in bk:\n",
    "        count +=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d005113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10010011\n"
     ]
    }
   ],
   "source": [
    "with bz2.BZ2File('../data/challenge/bookings.csv.bz2') as bk:\n",
    "    for i in bk:\n",
    "        count +=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f620bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = pd.read_csv('./bookings_sample.csv.bz2', sep= '^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd2fe200",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePathOrBuffer', sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, squeeze=False, prefix=<no_default>, mangle_dupe_cols=True, dtype: 'DtypeArg | None' = None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: 'str' = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors: 'str | None' = 'strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: 'StorageOptions' = None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "           This behavior was previously only the case for ``engine=\"python\"``.\n",
      "    \n",
      "        .. versionchanged:: 1.3.0\n",
      "    \n",
      "           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
      "           influence on how encoding errors are handled.\n",
      "    \n",
      "    encoding_errors : str, optional, default \"strict\"\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, default ``None``\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will be dropped from the DataFrame that is\n",
      "        returned.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    warn_bad_lines : bool, default ``None``\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'}, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "    \n",
      "            - 'error', raise an Exception when a bad line is encountered.\n",
      "            - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
      "            - 'skip', skip bad lines without raising or warning when they are encountered.\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "        'legacy' for the original lower precision pandas converter, and\n",
      "        'round_trip' for the round-trip converter.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "    \n",
      "        .. versionadded:: 1.2\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42f507ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_date</th>\n",
       "      <th>source</th>\n",
       "      <th>pos_ctry</th>\n",
       "      <th>pos_iata</th>\n",
       "      <th>pos_oid</th>\n",
       "      <th>rloc</th>\n",
       "      <th>cre_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>distance</th>\n",
       "      <th>dep_port</th>\n",
       "      <th>...</th>\n",
       "      <th>route</th>\n",
       "      <th>carrier</th>\n",
       "      <th>bkg_class</th>\n",
       "      <th>cab_class</th>\n",
       "      <th>brd_time</th>\n",
       "      <th>off_time</th>\n",
       "      <th>pax</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>oid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-03-05 00:00:00</td>\n",
       "      <td>1A</td>\n",
       "      <td>DE</td>\n",
       "      <td>a68dd7ae953c8acfb187a1af2dcbe123</td>\n",
       "      <td>1a11ae49fcbf545fd2afc1a24d88d2b7</td>\n",
       "      <td>ea65900e72d71f4626378e2ebd298267</td>\n",
       "      <td>2013-02-22 00:00:00</td>\n",
       "      <td>1708</td>\n",
       "      <td>0</td>\n",
       "      <td>ZRH</td>\n",
       "      <td>...</td>\n",
       "      <td>LHRZRH</td>\n",
       "      <td>VI</td>\n",
       "      <td>T</td>\n",
       "      <td>Y</td>\n",
       "      <td>2013-03-07 08:50:00</td>\n",
       "      <td>2013-03-07 11:33:37</td>\n",
       "      <td>-1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>1A</td>\n",
       "      <td>US</td>\n",
       "      <td>e612b9eeeee6f17f42d9b0d3b79e75ca</td>\n",
       "      <td>7437560d8f276d6d05eeb806d9e7edee</td>\n",
       "      <td>737295a86982c941f1c2da9a46a14043</td>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>135270</td>\n",
       "      <td>0</td>\n",
       "      <td>SAL</td>\n",
       "      <td>...</td>\n",
       "      <td>SALATLCLT</td>\n",
       "      <td>NV</td>\n",
       "      <td>L</td>\n",
       "      <td>Y</td>\n",
       "      <td>2013-04-12 13:04:00</td>\n",
       "      <td>2013-04-12 22:05:40</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>1A</td>\n",
       "      <td>US</td>\n",
       "      <td>e612b9eeeee6f17f42d9b0d3b79e75ca</td>\n",
       "      <td>7437560d8f276d6d05eeb806d9e7edee</td>\n",
       "      <td>737295a86982c941f1c2da9a46a14043</td>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>135270</td>\n",
       "      <td>0</td>\n",
       "      <td>SAL</td>\n",
       "      <td>...</td>\n",
       "      <td>CLTATLSAL</td>\n",
       "      <td>NV</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>2013-07-15 07:00:00</td>\n",
       "      <td>2013-07-15 11:34:51</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>1A</td>\n",
       "      <td>AU</td>\n",
       "      <td>0f984b3bb6bd06661c95529bbd6193bc</td>\n",
       "      <td>36472c6dbaf7afec9136ac40364e2794</td>\n",
       "      <td>5ecf00fdcbcec761c43dc7285253d0c1</td>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>30885</td>\n",
       "      <td>0</td>\n",
       "      <td>AKL</td>\n",
       "      <td>...</td>\n",
       "      <td>AKLHKGSVO</td>\n",
       "      <td>XK</td>\n",
       "      <td>G</td>\n",
       "      <td>Y</td>\n",
       "      <td>2013-04-24 23:59:00</td>\n",
       "      <td>2013-04-25 16:06:31</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>SYDA82546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>1A</td>\n",
       "      <td>AU</td>\n",
       "      <td>0f984b3bb6bd06661c95529bbd6193bc</td>\n",
       "      <td>36472c6dbaf7afec9136ac40364e2794</td>\n",
       "      <td>5ecf00fdcbcec761c43dc7285253d0c1</td>\n",
       "      <td>2013-03-26 00:00:00</td>\n",
       "      <td>30885</td>\n",
       "      <td>0</td>\n",
       "      <td>AKL</td>\n",
       "      <td>...</td>\n",
       "      <td>SVOHKGAKL</td>\n",
       "      <td>XK</td>\n",
       "      <td>G</td>\n",
       "      <td>Y</td>\n",
       "      <td>2013-05-14 20:15:00</td>\n",
       "      <td>2013-05-16 10:44:50</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>SYDA82546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   act_date             source  pos_ctry                          pos_iata  \\\n",
       "0  2013-03-05 00:00:00  1A      DE        a68dd7ae953c8acfb187a1af2dcbe123   \n",
       "1  2013-03-26 00:00:00  1A      US        e612b9eeeee6f17f42d9b0d3b79e75ca   \n",
       "2  2013-03-26 00:00:00  1A      US        e612b9eeeee6f17f42d9b0d3b79e75ca   \n",
       "3  2013-03-26 00:00:00  1A      AU        0f984b3bb6bd06661c95529bbd6193bc   \n",
       "4  2013-03-26 00:00:00  1A      AU        0f984b3bb6bd06661c95529bbd6193bc   \n",
       "\n",
       "                          pos_oid                      rloc            \\\n",
       "0  1a11ae49fcbf545fd2afc1a24d88d2b7  ea65900e72d71f4626378e2ebd298267   \n",
       "1  7437560d8f276d6d05eeb806d9e7edee  737295a86982c941f1c2da9a46a14043   \n",
       "2  7437560d8f276d6d05eeb806d9e7edee  737295a86982c941f1c2da9a46a14043   \n",
       "3  36472c6dbaf7afec9136ac40364e2794  5ecf00fdcbcec761c43dc7285253d0c1   \n",
       "4  36472c6dbaf7afec9136ac40364e2794  5ecf00fdcbcec761c43dc7285253d0c1   \n",
       "\n",
       "   cre_date             duration  distance  dep_port  ...  route            \\\n",
       "0  2013-02-22 00:00:00      1708         0  ZRH       ...  LHRZRH            \n",
       "1  2013-03-26 00:00:00    135270         0  SAL       ...  SALATLCLT         \n",
       "2  2013-03-26 00:00:00    135270         0  SAL       ...  CLTATLSAL         \n",
       "3  2013-03-26 00:00:00     30885         0  AKL       ...  AKLHKGSVO         \n",
       "4  2013-03-26 00:00:00     30885         0  AKL       ...  SVOHKGAKL         \n",
       "\n",
       "  carrier  bkg_class  cab_class  brd_time             off_time            pax  \\\n",
       "0      VI  T          Y          2013-03-07 08:50:00  2013-03-07 11:33:37  -1   \n",
       "1      NV  L          Y          2013-04-12 13:04:00  2013-04-12 22:05:40   1   \n",
       "2      NV  U          Y          2013-07-15 07:00:00  2013-07-15 11:34:51   1   \n",
       "3      XK  G          Y          2013-04-24 23:59:00  2013-04-25 16:06:31   1   \n",
       "4      XK  G          Y          2013-05-14 20:15:00  2013-05-16 10:44:50   1   \n",
       "\n",
       "   year month  oid        \n",
       "0  2013     3  NULL       \n",
       "1  2013     3  NULL       \n",
       "2  2013     3  NULL       \n",
       "3  2013     3  SYDA82546  \n",
       "4  2013     3  SYDA82546  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9beada3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(book.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
